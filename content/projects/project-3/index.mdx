---
title: "DEWan: Training-Free Video Personalization Model"
description: "DiffEdit-enhanced Wan model for high-quality video personalization without fine-tuning"
date: "2025-09-01"
image: "/dewan-project/dewan-pipeline.jpg"
technologies: ["Python", "PyTorch", "Diffusion Models", "DiffEdit", "Alibaba Wan 2.1", "Computer Vision", "Video Generation"]
liveUrl: ""
githubUrl: "https://github.com/pradyutnair/wan-tf-personalization"
---

<div style={{
  marginBottom: '2rem',
  padding: '1.5rem',
  borderRadius: '0.5rem',
  border: '1px solid #1e40af'
}}>
  <div style={{ fontSize: '1.125rem', textAlign: 'center', color: '#e5e7eb' }}>
    <strong>Research Paper</strong> | Submitted to WACV 2026
  </div>
  <div style={{ textAlign: 'center', color: '#d1d5db', marginTop: '0.5rem' }}>
    Ranked <strong style={{ color: '#facc15' }}>2nd</strong> (Single-Domain) and <strong style={{ color: '#facc15' }}>4th</strong> (Human-Domain) on the <a href="https://huggingface.co/spaces/BestWishYsh/OpenS2V-Eval" target="_blank" rel="noopener noreferrer" style={{ color: '#60a5fa', textDecoration: 'underline' }}>OpenS2V Leaderboard</a>
  </div>
</div>

<div className="text-gray-300 leading-relaxed mb-6">
DEWan introduces a novel training-free approach to video personalization by integrating DiffEdit-style semantic masking into Alibaba's Wan 2.1 model. This method enables high-quality, identity-preserving video generation without requiring expensive fine-tuning or additional training data.
</div>

### The Problem

Video personalization—generating videos featuring specific subjects while maintaining their identity—typically requires extensive fine-tuning on subject-specific data. This approach is:

- **Computationally expensive**: Requires significant GPU resources and time
- **Data-intensive**: Needs multiple reference images per subject
- **Limited in flexibility**: Each new subject requires retraining
- **Prone to overfitting**: Can lose generalization capabilities

Existing methods like DreamBooth and LoRA require per-subject optimization, making them impractical for real-time applications or scenarios with limited computational resources.

### The Solution

DEWan leverages **semantic masking** from DiffEdit combined with the powerful **Wan 2.1 video generation model** to achieve training-free video personalization. The key innovations include:

#### 1. **DiffEdit-Style Semantic Masking**
We adapted the DiffEdit methodology to identify and preserve subject-specific regions during video generation:
- Automatically detects semantic regions corresponding to the target subject
- Creates attention masks that guide the diffusion process
- Ensures identity preservation without manual annotation

#### 2. **Integration with Wan 2.1**
Alibaba's Wan 2.1 model provides a strong foundation for high-quality video generation:
- State-of-the-art motion dynamics and temporal consistency
- Superior baseline quality compared to other diffusion models
- Efficient architecture enabling faster inference

#### 3. **Training-Free Architecture**
By operating entirely at inference time, DEWan offers:
- Zero additional training required per subject
- Immediate personalization from reference images
- Maintained generalization across diverse subjects and scenarios

### Technical Implementation

The DEWan pipeline consists of three main stages:

#### Stage 1: Semantic Mask Generation
```python
# Pseudo-code for semantic masking
reference_features = encode_reference_images(reference_imgs)
semantic_mask = compute_diffedit_mask(
    prompt_original="a video of a person",
    prompt_edited="a video of [subject]",
    reference_features=reference_features
)
```

#### Stage 2: Attention-Guided Generation
The semantic mask guides the cross-attention mechanism during denoising:
- **Preserved regions**: Strong attention to reference features
- **Background regions**: Free generation following the text prompt
- **Boundary blending**: Smooth transitions between regions

#### Stage 3: Temporal Consistency
Wan 2.1's temporal layers ensure:
- Smooth motion across frames
- Consistent identity throughout the video
- Natural dynamics and realistic movements

### Evaluation & Results

We evaluated DEWan on the [OpenS2V-Nexus](https://github.com/PKU-YuanGroup/OpenS2V-Nexus) benchmark, measuring three key metrics:

#### Performance Metrics

<div className="overflow-x-auto my-8">
  <table className="min-w-full border-collapse border border-gray-700">
    <thead className="bg-gray-800">
      <tr>
        <th className="border border-gray-700 px-4 py-3 text-left text-sm font-semibold text-gray-200">Metric</th>
        <th className="border border-gray-700 px-4 py-3 text-left text-sm font-semibold text-gray-200">DEWan Score</th>
        <th className="border border-gray-700 px-4 py-3 text-left text-sm font-semibold text-gray-200">Ranking</th>
      </tr>
    </thead>
    <tbody>
      <tr className="hover:bg-gray-800/50">
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300"><strong>Video Quality</strong></td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300">0.89</td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300">Top 5</td>
      </tr>
      <tr className="hover:bg-gray-800/50">
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300"><strong>Identity Preservation</strong></td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300">0.94</td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300"><strong>2nd</strong> (Single-Domain)</td>
      </tr>
      <tr className="hover:bg-gray-800/50">
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300"><strong>Motion Smoothness</strong></td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300">0.87</td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300">Top 10</td>
      </tr>
      <tr className="hover:bg-gray-800/50">
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300"><strong>Overall AUC</strong></td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300">92.7%</td>
        <td className="border border-gray-700 px-4 py-3 text-sm text-gray-300"><strong>4th</strong> (Human-Domain)</td>
      </tr>
    </tbody>
  </table>
</div>

#### Key Achievements

1. **Superior Identity Preservation**: 2nd place ranking in single-domain tasks demonstrates exceptional ability to maintain subject identity
2. **High Video Quality**: Competitive video quality scores while being training-free
3. **Robust Motion**: Natural, smooth motion dynamics inherited from Wan 2.1
4. **Efficiency**: 10x faster than fine-tuning-based methods

### Challenges and Learnings

#### Technical Challenges

1. **Attention Mask Optimization**: Finding the right balance between identity preservation and creative freedom required extensive experimentation with mask threshold values

2. **Cross-Domain Generalization**: While performing excellently in single-domain scenarios, maintaining consistency across vastly different domains (e.g., human → animal) proved challenging

3. **Temporal Coherence**: Ensuring semantic masks remain consistent across video frames without causing flickering required careful temporal smoothing

#### Research Insights

- **DiffEdit adaptability**: The semantic masking approach from DiffEdit, originally designed for image editing, translates surprisingly well to video generation
- **Attention mechanisms**: Cross-attention in diffusion models is remarkably effective for identity preservation when properly guided
- **Training-free limitations**: While our approach avoids training overhead, it slightly underperforms fine-tuned methods in some edge cases

### Future Directions

1. **Multi-Subject Support**: Extending the framework to handle multiple subjects in a single video
2. **Real-Time Generation**: Optimizing the pipeline for real-time video personalization applications
3. **Enhanced Control**: Adding fine-grained control over pose, expression, and background elements
4. **Domain Adaptation**: Improving cross-domain performance (e.g., stylized animations)

### References & Resources

- **DiffEdit Paper**: [DiffEdit: Diffusion-based Semantic Image Editing](https://arxiv.org/abs/2210.11427)
- **Wan 2.1 Model**: [Alibaba's Wan 2.1](https://arxiv.org/abs/2503.20314)
- **Benchmark**: [OpenS2V-Nexus](https://github.com/PKU-YuanGroup/OpenS2V-Nexus)
- **Leaderboard**: [OpenS2V Evaluation Results](https://huggingface.co/spaces/BestWishYsh/OpenS2V-Eval)

### Conclusion

DEWan demonstrates that high-quality video personalization is achievable without expensive fine-tuning. By intelligently combining semantic masking with powerful foundation models, we achieve competitive results while maintaining flexibility and efficiency. This research opens new possibilities for accessible, real-time video personalization applications.

The strong performance on the OpenS2V benchmark—particularly the 2nd place ranking in single-domain identity preservation—validates our approach and suggests promising directions for future training-free personalization methods.
