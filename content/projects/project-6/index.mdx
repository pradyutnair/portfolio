---
title: "Concept-Aware Geolocation"
description: "An interpretable geolocation system using Hierarchical Concept Bottleneck Models with vision-language foundations for explaining location predictions."
date: "2024-12-15"
image: "/geoguessr-cbm-project/arch3.png"
technologies: ["Python", "PyTorch", "CLIP", "GeoCLIP", "FastAPI", "TypeScript", "LaTeX"]
githubUrl: "https://github.com/pradyutnair/geoguessr-cbm"
---

GeoGuessr players instinctively know *why* they place a marker somewhere—they spot distinctive road markings, recognize architectural styles, or identify unique vegetation. But machine learning geolocation models operate as black boxes, making predictions without explaining their reasoning. Concept-Aware Geolocation bridges this gap by forcing all location predictions to flow through human-interpretable semantic concepts.

## The Motivation

Existing geolocation systems like PlaNet and PIGEON achieve impressive accuracy but provide no insight into their decision-making process. When a model predicts "this image is in Brazil," we can't ask what visual features led to that conclusion. This opacity is problematic for safety-critical applications where understanding the reasoning is as important as the prediction itself.

## The Approach

I built a three-stage curriculum learning pipeline that combines the power of vision-language models with the interpretability of concept bottlenecks. The key insight: instead of mapping images directly to GPS coordinates, we first predict semantic concepts (like "Urban Street," "Suburban Road," "License Plates visible") and then use those concepts to predict location.

### Stage 0: Domain Contrastive Pretraining

The foundation starts with StreetCLIP and GeoCLIP—models that have learned powerful geographic representations from millions of images. But they're not optimized for the challenging GeoGuessr "meta" locations curated by the community. I developed a contrastive pretraining stage with a custom GPS adapter network (512d→768d) that aligns GeoCLIP's location embeddings with StreetCLIP's image features, creating a unified embedding space where visually similar locations cluster together.

### Stage 1: Text-Anchored Concept Learning

Rather than learning concept prototypes from scratch, I initialize them from CLIP text embeddings. This means the prototype for "Urban Street" starts close to CLIP's understanding of that phrase, then adapts through learnable residuals to capture street-view specific patterns. The model learns ~100 fine-grained child concepts (like "Highway," "Roundabout," "Unique car") that roll up into ~15 coarse parent concepts (like "Urban," "Rural," "Infrastructure").

What's clever here is the hierarchical consistency loss—the model learns that if it predicts "Highway" (child), it should also predict "Infrastructure" (parent). This creates a structured semantic understanding that mirrors how humans reason about locations.

### Stage 2: Gated Fusion for Geolocation

The final stage predicts coordinates by combining concept embeddings with spatial image information. A cross-attention mechanism lets the concept embedding query image patches to extract relevant spatial context—essentially asking "which parts of the image support this concept prediction?"

The real innovation is the learned gate. Instead of naively concatenating features, the model learns per-dimension weights that balance concept vs. spatial information. Gate values near 1 mean "rely on concepts" (good for architecture, road markings), while values near 0 mean "rely on spatial patterns" (good for vegetation, lighting). Median gate value around 0.58 shows the model uses a balanced combination, with dimensional specialization revealing which geographic features depend on semantic understanding.

## The Data

I trained on 43,040 street view images from learnablemeta.com—a treasure trove of challenging GeoGuessr locations. Each image has GPS coordinates, hierarchical concept labels, and country information. The model uses adaptive geocells generated through per-country K-means clustering, producing 1,048 cells that provide finer granularity in data-dense regions (Europe, South America, East Asia) while maintaining global coverage.

## Results

![Architecture Diagram](/geoguessr-cbm-project/arch3.png)

The model achieves competitive performance while providing full interpretability through a concept bottleneck architecture.

### In-Distribution Performance

Evaluated on the test split (4,304 samples), the finetuned variant with gated fusion achieves the best median error of 126 km.

<div style={{overflowX: 'auto', margin: '1.5rem 0'}}>
<table style={{width: '100%', borderCollapse: 'collapse', fontSize: '0.875rem'}}>
<thead>
<tr style={{borderBottom: '2px solid #374151'}}>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Variant</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Mode</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Median (km)</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Mean (km)</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Cell Acc</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>City</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Region</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Country</th>
</tr>
</thead>
<tbody>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Vanilla</td>
<td style={{padding: '0.5rem'}}>Both</td>
<td style={{padding: '0.5rem'}}>133.2</td>
<td style={{padding: '0.5rem'}}>713.8</td>
<td style={{padding: '0.5rem'}}>0.454</td>
<td style={{padding: '0.5rem'}}>0.215</td>
<td style={{padding: '0.5rem'}}>0.574</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.830</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Vanilla</td>
<td style={{padding: '0.5rem'}}>Concept</td>
<td style={{padding: '0.5rem'}}>139.0</td>
<td style={{padding: '0.5rem'}}>745.9</td>
<td style={{padding: '0.5rem'}}>0.451</td>
<td style={{padding: '0.5rem'}}>0.195</td>
<td style={{padding: '0.5rem'}}>0.566</td>
<td style={{padding: '0.5rem'}}>0.824</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Vanilla</td>
<td style={{padding: '0.5rem'}}>Image</td>
<td style={{padding: '0.5rem'}}>222.0</td>
<td style={{padding: '0.5rem'}}>1070.5</td>
<td style={{padding: '0.5rem'}}>0.374</td>
<td style={{padding: '0.5rem'}}>0.175</td>
<td style={{padding: '0.5rem'}}>0.482</td>
<td style={{padding: '0.5rem'}}>0.753</td>
</tr>
<tr style={{borderBottom: '1px solid #374151', backgroundColor: '#1f2937'}}>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>Finetuned</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>Both</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>126.0</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>684.6</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.449</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.232</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.578</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.829</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Finetuned</td>
<td style={{padding: '0.5rem'}}>Concept</td>
<td style={{padding: '0.5rem'}}>137.0</td>
<td style={{padding: '0.5rem'}}>688.6</td>
<td style={{padding: '0.5rem'}}>0.443</td>
<td style={{padding: '0.5rem'}}>0.227</td>
<td style={{padding: '0.5rem'}}>0.564</td>
<td style={{padding: '0.5rem'}}>0.822</td>
</tr>
<tr>
<td style={{padding: '0.5rem'}}>Finetuned</td>
<td style={{padding: '0.5rem'}}>Image</td>
<td style={{padding: '0.5rem'}}>154.0</td>
<td style={{padding: '0.5rem'}}>790.5</td>
<td style={{padding: '0.5rem'}}>0.430</td>
<td style={{padding: '0.5rem'}}>0.202</td>
<td style={{padding: '0.5rem'}}>0.546</td>
<td style={{padding: '0.5rem'}}>0.806</td>
</tr>
</tbody>
</table>
</div>

**Key Findings:**
- Concept-only predictions achieve 137.0 km versus 126.0 km for the full model—within 10%—validating that interpretability doesn't sacrifice much accuracy
- Image-only mode performs notably worse (154.0 km), indicating the concept bottleneck provides regularization benefits
- The gated fusion achieves the best performance by adaptively combining concept and spatial information
- Country-level accuracy exceeds 80% across all configurations

### Out-of-Distribution Performance

Evaluated on a publicly available GeoGuessr dataset containing 5,477 street view images to assess generalization beyond the training distribution.

<div style={{overflowX: 'auto', margin: '1.5rem 0'}}>
<table style={{width: '100%', borderCollapse: 'collapse', fontSize: '0.875rem'}}>
<thead>
<tr style={{borderBottom: '2px solid #374151'}}>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Variant</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Mode</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Median (km)</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Mean (km)</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Cell Acc</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>City</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Region</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Country</th>
</tr>
</thead>
<tbody>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Vanilla</td>
<td style={{padding: '0.5rem'}}>Both</td>
<td style={{padding: '0.5rem'}}>391.5</td>
<td style={{padding: '0.5rem'}}>1643.7</td>
<td style={{padding: '0.5rem'}}>0.234</td>
<td style={{padding: '0.5rem'}}>0.032</td>
<td style={{padding: '0.5rem'}}>0.323</td>
<td style={{padding: '0.5rem'}}>0.673</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Vanilla</td>
<td style={{padding: '0.5rem'}}>Concept</td>
<td style={{padding: '0.5rem'}}>417.6</td>
<td style={{padding: '0.5rem'}}>1788.8</td>
<td style={{padding: '0.5rem'}}>0.222</td>
<td style={{padding: '0.5rem'}}>0.033</td>
<td style={{padding: '0.5rem'}}>0.310</td>
<td style={{padding: '0.5rem'}}>0.644</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Vanilla</td>
<td style={{padding: '0.5rem'}}>Image</td>
<td style={{padding: '0.5rem'}}>448.4</td>
<td style={{padding: '0.5rem'}}>1894.7</td>
<td style={{padding: '0.5rem'}}>0.217</td>
<td style={{padding: '0.5rem'}}>0.026</td>
<td style={{padding: '0.5rem'}}>0.301</td>
<td style={{padding: '0.5rem'}}>0.631</td>
</tr>
<tr style={{borderBottom: '1px solid #374151', backgroundColor: '#1f2937'}}>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>Finetuned</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>Both</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>349.9</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>1616.9</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.265</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.034</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.360</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>0.688</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Finetuned</td>
<td style={{padding: '0.5rem'}}>Concept</td>
<td style={{padding: '0.5rem'}}>381.8</td>
<td style={{padding: '0.5rem'}}>1670.2</td>
<td style={{padding: '0.5rem'}}>0.255</td>
<td style={{padding: '0.5rem'}}>0.032</td>
<td style={{padding: '0.5rem'}}>0.340</td>
<td style={{padding: '0.5rem'}}>0.665</td>
</tr>
<tr style={{borderBottom: '1px solid #374151'}}>
<td style={{padding: '0.5rem'}}>Finetuned</td>
<td style={{padding: '0.5rem'}}>Image</td>
<td style={{padding: '0.5rem'}}>387.0</td>
<td style={{padding: '0.5rem'}}>1709.8</td>
<td style={{padding: '0.5rem'}}>0.242</td>
<td style={{padding: '0.5rem'}}>0.030</td>
<td style={{padding: '0.5rem'}}>0.336</td>
<td style={{padding: '0.5rem'}}>0.665</td>
</tr>
<tr>
<td style={{padding: '0.5rem', fontStyle: 'italic', color: '#9ca3af'}}>GeoCLIP (baseline)</td>
<td style={{padding: '0.5rem'}}>—</td>
<td style={{padding: '0.5rem'}}>1015.8</td>
<td style={{padding: '0.5rem'}}>3190.6</td>
<td style={{padding: '0.5rem'}}>—</td>
<td style={{padding: '0.5rem'}}>0.027</td>
<td style={{padding: '0.5rem'}}>0.160</td>
<td style={{padding: '0.5rem'}}>0.424</td>
</tr>
</tbody>
</table>
</div>

**Key Findings:**
- The finetuned variant reduces median error to 349.9 km compared to 391.5 km for vanilla (~11% improvement)
- Concept-only remains competitive at 381.8 km, demonstrating robust generalization
- Significantly outperforms the baseline GeoCLIP model (1015.8 km median error)

### Live GeoGuessr Performance

Deployed as an automated bot playing 25 live GeoGuessr rounds across 5 games:

<div style={{overflowX: 'auto', margin: '1.5rem 0'}}>
<table style={{width: '100%', borderCollapse: 'collapse', fontSize: '0.875rem'}}>
<thead>
<tr style={{borderBottom: '2px solid #374151'}}>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Method</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Median Distance (km)</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Mean Distance (km)</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Median Score</th>
<th style={{padding: '0.5rem', textAlign: 'left', fontWeight: '600'}}>Mean Score</th>
</tr>
</thead>
<tbody>
<tr style={{borderBottom: '1px solid #374151', backgroundColor: '#1f2937'}}>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>Our Model</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>549.8</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>1527.0</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>3459</td>
<td style={{padding: '0.5rem', fontWeight: 'bold'}}>3326</td>
</tr>
<tr>
<td style={{padding: '0.5rem'}}>Human Player</td>
<td style={{padding: '0.5rem'}}>849.0</td>
<td style={{padding: '0.5rem'}}>2643.0</td>
<td style={{padding: '0.5rem'}}>2830</td>
<td style={{padding: '0.5rem'}}>2665</td>
</tr>
</tbody>
</table>
</div>

The model outperforms the human player with lower median error and higher GeoGuessr scores, demonstrating that the concept bottleneck doesn't compromise practical performance in real-time game settings.

## Interpretability in Action

![Interpretability Example](/geoguessr-cbm-project/round_09_110825_concepts.png)

The interpretability features reveal the model's reasoning process:

- **Patch-level attention maps** show exactly which image regions support each prediction (vehicles, license plates, road geometry)
- **Hierarchical concept predictions** give both specific ("Unique car," "License Plates") and coarse ("car_meta," "landscape_savannah") level insights
- **Gate value analysis** reveals dimensional specialization—some dimensions are concept-dominated, others spatial-dominated

For example, when identifying a West African location, the model attends to unique vehicles and license plates while predicting concepts like "Unique car" and "License Plates," with parent concepts indicating "car_meta" and "landscape_savannah" categories. The gate value of 0.431 shows balanced concept-spatial reasoning.

## Technical Stack

Built with PyTorch, leveraging CLIP, StreetCLIP, and GeoCLIP as foundation models. Inference served via FastAPI with an automated bot that integrates directly with GeoGuessr's game API for real-time evaluation. The complete research paper documents methodology, experiments, and analysis in LaTeX.

## Why This Matters

This work demonstrates that interpretability doesn't require sacrificing accuracy. By enforcing a concept bottleneck, we get predictions that can be inspected, understood, and even debugged. The learned gates reveal which aspects of geolocation rely on semantic concepts versus spatial patterns, providing insights into how the model reasoning about geographic features.

Future directions include automatic concept discovery, temporal reasoning for video geolocation, and interactive explanation interfaces that communicate model reasoning to end users.
